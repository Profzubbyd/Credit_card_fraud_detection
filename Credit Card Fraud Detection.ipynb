{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jyr-KFp4wgd6"
   },
   "outputs": [],
   "source": [
    "# Imported Libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "mDQZX0RtwgeB",
    "outputId": "6f31666a-6b87-4446-93cb-ba2b7adf3abb"
   },
   "outputs": [],
   "source": [
    "# path = \"/content/drive/MyDrive/Datasets/creditcard.csv\"\n",
    "# df = pd.read_csv(path)\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3MB_glJwgeG",
    "outputId": "48cd575d-c3b9-4008-a04e-94f0da639962"
   },
   "outputs": [],
   "source": [
    "# Show column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm0vLXHeL6Mu"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-zPJE0MPUWM"
   },
   "source": [
    "We start by checking for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9q3tGcHwgeF",
    "outputId": "1c38239c-8e51-4cee-ff23-bd969964c5c8"
   },
   "outputs": [],
   "source": [
    "# Check for Null Values!\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "di54MErcwgeH",
    "outputId": "40d63125-ec51-40c7-fa2c-e6fc49b04d8d"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the transacton types\n",
    "color = [\"blue\", \"red\"]\n",
    "sns.countplot(x='Class', data=df, palette = color)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distributions', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z9MHOceMYMP"
   },
   "source": [
    "This plot shows us that the dataset is largely unbalanced. This will make the model to be biased towards 0 (non-fraudulent transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BflFpHa3wgeI",
    "outputId": "379a3ba0-d377-4cc5-9eb5-a9b01a87d532"
   },
   "outputs": [],
   "source": [
    "# The classes are heavily skewed\n",
    "print('Non-Fraudulent:', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Fraudulent:', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv5hsFGvPABB"
   },
   "source": [
    "Next, we scale the Amount and Time column since those are the only columns that aren't scaled in our dataset. We can see this from the describe table below.\n",
    "Another reason we need to scale is because we will be using some distance based algorithms to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dUNGSUR3wgeD",
    "outputId": "d8e9d10f-9861-42ea-d0b0-6272836a1b1a"
   },
   "outputs": [],
   "source": [
    "# Extract statistical details of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRDpf9nHUIOr"
   },
   "source": [
    "We use RobustScaler to perform the scaling which is less prone to outliers because it uses the median and Inter Quartile Range (IQR) to scale the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1VlT0UxwgeK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "# We use RobustScaler that's less prone to Outliers\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "TCdQl7slwgeL",
    "outputId": "7a22f645-036f-4e49-ae14-f5889f1a5861"
   },
   "outputs": [],
   "source": [
    "# Every column is Scaled!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2ULoSxyVCrL"
   },
   "source": [
    "Now, we perform cross validation on the main (unbalanced) dataset to bring out test values that won't be affected by the undersampling. \n",
    "\n",
    "StratifiedkFold ensures they maintain the class proportion (99.83 : 0.17) in each of the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hD5jrKMawgeN",
    "outputId": "59b0fcfc-dc20-4c33-d42e-8af0432d649b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print('Non-Fraudulent:', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Fraudulent:', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "ss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in ss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# We would still get X_train and y_train for the undersample data that's why we are using original to distinguish.\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "# Turn them into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(\"Train Distribution:\", train_counts_label/ len(original_ytrain))\n",
    "print(\"Test Distribution:\", test_counts_label/ len(original_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yp1-iFfMYWoD"
   },
   "source": [
    "### UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6V0FmUiwgeQ",
    "outputId": "c5f1bc08-c392-4ef8-91ce-1cdae36414db"
   },
   "outputs": [],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixOJMTyhasIV"
   },
   "source": [
    "Since the classes are highly skewed towards the non-fraudulent transactions, we use undersampling to make them equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "55Fu4zKuwgeR",
    "outputId": "ed34ec32-fb8a-44d2-e5b7-d88d00e96dff"
   },
   "outputs": [],
   "source": [
    "# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n",
    "# Lets shuffle the data before creating the subsamples\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Amount of fraud classes 492 rows.\n",
    "fraud = df.loc[df['Class'] == 1]\n",
    "non_fraud = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "sub_sampled_df = pd.concat([fraud, non_fraud])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = sub_sampled_df.sample(frac=1, random_state=4)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGSV1hdhbf0y"
   },
   "source": [
    "Using frac=1 in df.sample returns the shuffled dataset completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AH895ZvgwgeS",
    "outputId": "7c898379-b203-484c-da56-9ff71af488f7"
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "vgfK6dX8wgeS",
    "outputId": "45c09d75-f220-402f-b4fa-84fce7050e07"
   },
   "outputs": [],
   "source": [
    "# Check Equal distribution\n",
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(new_df['Class'].value_counts()/len(new_df))\n",
    "\n",
    "sns.countplot(x='Class', data=new_df, palette=color)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGFpEXhkb5do"
   },
   "source": [
    "Now we see that the dataset (new_df) is equally distributed amongst the classes.\n",
    "\n",
    "Next, we check if any variable is strongly correlated with the Class column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "mGG_rhDlwgeT",
    "outputId": "bb89674f-f24b-41a8-b85d-cc8319b6e0af"
   },
   "outputs": [],
   "source": [
    "# Show correlation matrix for the subsample dataframe\n",
    "sns.heatmap(new_df.corr(), cmap='coolwarm_r', annot_kws={'size':20})\n",
    "plt.title('UnderSample Correlation Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NdF1HOBwgeT"
   },
   "source": [
    "Negative Correlations: V16, V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n",
    "\n",
    "Positive Correlations: V2, V4 and V11 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "talvZ1vOwgeX"
   },
   "source": [
    "## Data Modeling For Undersampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk9oQMRSwgeZ"
   },
   "outputs": [],
   "source": [
    "# Undersampling before cross validating\n",
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n",
    "# Our data is already scaled we should split our training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Turn the values into an array for feeding the classification algorithms.\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt7TgLW7wgeZ"
   },
   "outputs": [],
   "source": [
    "# We implement four classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FB_bUe3wgeZ",
    "outputId": "44a942af-2910-4e3d-b035-0762642b16b1"
   },
   "outputs": [],
   "source": [
    "# Now, we calculate the cross validation score with each classifier.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmAXq1MWg6MX"
   },
   "source": [
    "We can see that Logistic Regression has the best cross_val_score, now we use GridSearchCV to find the best parameters for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFCfhr4Ywgea",
    "outputId": "bd11f161-7048-4263-9dd4-f8c6ee26a222"
   },
   "outputs": [],
   "source": [
    "# We calculate the cross validation scores using the best parameters\n",
    "\n",
    "log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "print('Logistic Regression Cross Validation Score:', round(log_reg_score.mean() * 100, 2), '%')\n",
    "\n",
    "knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
    "print('Knears Neighbors Cross Validation Score:', round(knears_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
    "print('Support Vector Classifier Cross Validation Score:', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "rf_score = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "print('RandomForest Classifier Cross Validation Score:', round(rf_score.mean() * 100, 2).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLTgaGNNlEj6"
   },
   "source": [
    "After using the best parameters, we still see that Logistic regression has the best cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khWQUemzwgeb"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV is used to determine the paremeters that gives the best predictive score for the classifiers.\n",
    "# Logistic Regression has the best Receiving Operating Characteristic score (ROC), meaning that LogisticRegression pretty accurately separates fraud and non-fraud transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJMSsVwKwgeb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "# Create a DataFrame with all the scores and the classifiers names.\n",
    "\n",
    "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
    "\n",
    "svc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "rf_pred = cross_val_predict(rf_clf, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-inyjQ-wgeb",
    "outputId": "bda8a3bd-cbc6-4dc4-bd1c-645ed72b5983"
   },
   "outputs": [],
   "source": [
    "# Calculate ROC scores\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
    "print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
    "print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
    "print('Random Forest Classifier: ', roc_auc_score(y_train, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPjAzy3jpGc2"
   },
   "source": [
    "We observe that Logistic Regression has the best cross_val_score and a very high Receiving Operating Characteristic score (ROC), meaning that LogisticRegression pretty accurately separates fraud and non-fraud transactions.\n",
    "\n",
    "So we would use the Logistic Regression Model when oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6if3Tfn1wgec"
   },
   "source": [
    "## SMOTE Technique (Over-Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2EKFwHqp419"
   },
   "source": [
    "Synthetic Minority Oversampling Technique (SMOTE) synthesizes new examples from the minority class to make the dataset balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofEhNwklu-z9"
   },
   "source": [
    "We are very interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions. The formulars of Accuracy, Precision and Recall are given below:\n",
    "*   Accuracy = (TP+TN)/total\n",
    "*   Precision = TP/(TP+FP)\n",
    "*   Recall = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVnZ-u2dwgee"
   },
   "outputs": [],
   "source": [
    "# SMOTE Technique (OverSampling) After splitting and Cross Validating\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "\n",
    "# This will be the data were we are going to \n",
    "Xsm_train, ysm_train = sm.fit_resample(original_Xtrain, original_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFxCFQVqwgee",
    "outputId": "0266e89d-5ca8-486b-ae8b-c7c480266e55"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg_sm = LogisticRegression()\n",
    "log_reg_sm.fit(Xsm_train, ysm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1djHBddwgee"
   },
   "source": [
    "### SMOTE Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "PtYwki1Iwgef",
    "outputId": "898067db-2a16-41b0-d47c-f1c2c13d632c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Logistic Regression fitted using SMOTE technique\n",
    "y_pred_log_reg = log_reg_sm.predict(X_test)\n",
    "\n",
    "# Other models fitted with UnderSampling\n",
    "y_pred_knear = knears_neighbors_sm.predict(X_test)\n",
    "y_pred_svc = svc_sm.predict(X_test)\n",
    "y_pred_rf = rf_sm.predict(X_test)\n",
    "\n",
    "\n",
    "log_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\n",
    "kneighbors_cf = confusion_matrix(y_test, y_pred_knear)\n",
    "svc_cf = confusion_matrix(y_test, y_pred_svc)\n",
    "rf_cf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 2,figsize=(22,10))\n",
    "\n",
    "\n",
    "sns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.Blues)\n",
    "ax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\n",
    "ax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.Blues)\n",
    "ax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\n",
    "ax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.Blues)\n",
    "ax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\n",
    "ax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(rf_cf, ax=ax[1][1], annot=True, cmap=plt.cm.Blues)\n",
    "ax[1][1].set_title(\"Random Forest Classifier \\n Confusion Matrix\", fontsize=14)\n",
    "ax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hctfS-XZwgeg",
    "outputId": "06925966-00dd-45d4-ef7c-36579bacaf34"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Logistic Regression:')\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "print('KNears Neighbors:')\n",
    "print(classification_report(y_test, y_pred_knear))\n",
    "\n",
    "print('Support Vector Classifier:')\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "print('Random Forest Classifier:')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuU1qyqKK9n1"
   },
   "source": [
    "Observe that the logistic regression using SMOTE has a higher accuracy than other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "DJa39u0dwgeh",
    "outputId": "0c1ee07e-3ba4-424f-b379-827d248a5000"
   },
   "outputs": [],
   "source": [
    "# Final Score in the main test set of logistic regression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression with Under-Sampling\n",
    "y_pred = log_reg.predict(X_test)\n",
    "undersample_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\n",
    "y_pred_sm = log_reg_sm.predict(original_Xtest)\n",
    "oversample_score = accuracy_score(original_ytest, y_pred_sm)\n",
    "\n",
    "\n",
    "d = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\n",
    "final_df = pd.DataFrame(data=d)\n",
    "\n",
    "# Move column\n",
    "score = final_df['Score']\n",
    "final_df.drop('Score', axis=1, inplace=True)\n",
    "final_df.insert(1, 'Score', score)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSs1BaqEwgei"
   },
   "source": [
    "## Neural Network (UnderSampling vs. OverSampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnaIH62uwgei"
   },
   "source": [
    "### UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lo5sjxb7wgei"
   },
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "\n",
    "undersample_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EDiRoZnwgej",
    "outputId": "72b13126-f758-47da-eba9-161661b8155b"
   },
   "outputs": [],
   "source": [
    "undersample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTARRgN2wgej"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "undersample_model.compile(opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cw0yphYIwgek",
    "outputId": "b90e5a36-21a3-47cf-fb09-529dd2dbfd1e"
   },
   "outputs": [],
   "source": [
    "undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDLEvzQINJuy"
   },
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFFoveeiwgel"
   },
   "outputs": [],
   "source": [
    "undersample_predictions = undersample_model.predict(original_Xtest, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSnFZKsbwgel"
   },
   "outputs": [],
   "source": [
    "predict_x = undersample_model.predict(original_Xtest, batch_size=200, verbose=0)\n",
    "undersample_fraud_predictions = np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "cXK6HKSCwgem",
    "outputId": "41ad89e0-2ab7-486c-9289-44142627a960"
   },
   "outputs": [],
   "source": [
    "undersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)\n",
    "actual_cm = confusion_matrix(original_ytest, original_ytest)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 8))\n",
    "\n",
    "sns.heatmap(undersample_cm, ax=ax1, annot=True, cmap=plt.cm.Reds)\n",
    "ax1.set_title(\"Random UnderSample \\n Confusion Matrix\", fontsize=14)\n",
    "\n",
    "sns.heatmap(actual_cm, ax=ax2, annot=True, cmap=plt.cm.Blues)\n",
    "ax2.set_title(\"Confusion Matrix \\n Of Original Test Set\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2ayLUDDwgem"
   },
   "source": [
    "### OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSKfIScAwgem"
   },
   "outputs": [],
   "source": [
    "n_inputs = Xsm_train.shape[1]\n",
    "\n",
    "oversample_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fFl-YwAwgen"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "oversample_model.compile(opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcY-DFS0wgeo",
    "outputId": "f3c8875f-af15-43f0-ec28-44397a2ef2e4"
   },
   "outputs": [],
   "source": [
    "oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NLxTXqpNZ-d"
   },
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOy-QHY9wgep"
   },
   "outputs": [],
   "source": [
    "oversample_predictions = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YaG9M0-wgep"
   },
   "outputs": [],
   "source": [
    "predict_x2 = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)\n",
    "oversample_fraud_predictions = np.argmax(predict_x2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "vF2YcvSVwgep",
    "outputId": "312e1266-ea22-4e4f-eed2-90261ba3f9e7"
   },
   "outputs": [],
   "source": [
    "oversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\n",
    "actual_cm = confusion_matrix(original_ytest, original_ytest)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 8))\n",
    "\n",
    "sns.heatmap(oversample_smote, ax=ax1, annot=True, cmap=plt.cm.Oranges)\n",
    "ax1.set_title(\"OverSample (SMOTE) \\n Confusion Matrix\", fontsize=14)\n",
    "\n",
    "sns.heatmap(actual_cm, ax=ax2, annot=True, cmap=plt.cm.Greens)\n",
    "ax2.set_title(\"Confusion Matrix \\n Of Original Test Set\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpBQz2b7N6TD"
   },
   "source": [
    "We can see that the Neural Network Model of the OverSampled Data did much better than the one applied to the UnderSampled Data. This could be because when undersampling there is loss of information which could have been used in model building."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Fm0vLXHeL6Mu"
   ],
   "name": "Group 5 - Final Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
